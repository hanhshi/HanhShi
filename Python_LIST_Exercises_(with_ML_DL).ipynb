{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hanhshi/HanhShi/blob/main/Python_LIST_Exercises_(with_ML_DL).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **10 Bài tập luyện `list` trong bối cảnh Machine Learning / Deep Learning**\n",
        "\n",
        "### **Bài tập 1: Tạo và Đệm (Padding) cho các Batch Dữ liệu**\n",
        "\n",
        "**Bối cảnh:** Trong Deep Learning, đặc biệt là với các bài toán về Ngôn ngữ tự nhiên (NLP), chúng ta thường xử lý các câu có độ dài khác nhau. Để đưa dữ liệu vào mô hình theo từng \"lô\" (batch) một cách hiệu quả, các câu trong cùng một batch cần phải có cùng độ dài. Kỹ thuật phổ biến là \"đệm\" (padding), tức là thêm một giá trị đặc biệt (ví dụ: `0`) vào cuối các câu ngắn hơn cho đến khi chúng bằng độ dài của câu dài nhất trong batch.\n",
        "\n",
        "**Ứng dụng thực tế:** Kỹ thuật này là bắt buộc khi huấn luyện các mô hình như RNN, LSTM, hay Transformers trên GPU, vì nó cho phép thực hiện các phép tính ma trận song song hóa trên toàn bộ batch."
      ],
      "metadata": {
        "id": "fPoQA8Sj9kxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_padded_batch(data, batch_size):\n",
        "    \"\"\"\n",
        "    Chia một danh sách các câu (list of lists) thành các batch có kích thước `batch_size`.\n",
        "    Trong mỗi batch, đệm tất cả các câu để chúng có cùng độ dài với câu dài nhất trong batch đó.\n",
        "    Giá trị đệm là 0.\n",
        "\n",
        "    Ví dụ: data = [[1,2], [3,4,5]], batch_size = 2\n",
        "    -> Câu dài nhất có độ dài 3.\n",
        "    -> Batch được trả về là: [[1, 2, 0], [3, 4, 5]]\n",
        "    \"\"\"\n",
        "    # TODO: Your code here\n",
        "    pass\n",
        "\n",
        "# Test Cases\n",
        "assert create_padded_batch([[1, 2], [3, 4, 5], [6]], 2) == [[[1, 2, 0], [3, 4, 5]], [[6, 0]]], \"Test 1.1 Failed\"\n",
        "assert create_padded_batch([[10], [20, 30], [40, 50, 60], [70]], 4) == [[[10, 0, 0], [20, 30, 0], [40, 50, 60], [70, 0, 0]]], \"Test 1.2 Failed\"\n",
        "assert create_padded_batch([[1]], 1) == [[[1]]], \"Test 1.3 Failed\"\n",
        "assert create_padded_batch([], 5) == [], \"Test 1.4 Failed\"\n",
        "assert create_padded_batch([[1, 1], [2, 2], [3, 3]], 2) == [[[1, 1], [2, 2]], [[3, 3]]], \"Test 1.5 Failed\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "vWkrVzFM9kxn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Bài tập 2: Chia Tập dữ liệu (Train-Validation-Test Split)**\n",
        "\n",
        "**Bối cảnh:** Một bước không thể thiếu trong bất kỳ quy trình Machine Learning nào là đánh giá hiệu suất của mô hình. Để làm điều này một cách khách quan, chúng ta phải chia tập dữ liệu ban đầu thành ba phần:\n",
        "1.  **Tập Huấn luyện (Training set):** Dùng để \"dạy\" cho mô hình.\n",
        "2.  **Tập Kiểm định (Validation set):** Dùng để tinh chỉnh các siêu tham số (hyperparameters) của mô hình.\n",
        "3.  **Tập Kiểm tra (Test set):** Dùng để đánh giá hiệu suất cuối cùng của mô hình trên dữ liệu mà nó chưa từng thấy.\n",
        "\n",
        "**Ứng dụng thực tế:** Việc phân chia này đảm bảo rằng chúng ta không đánh giá mô hình trên chính dữ liệu mà nó đã học, giúp tránh \"học vẹt\" (overfitting) và đưa ra kết luận đáng tin cậy."
      ],
      "metadata": {
        "id": "5UQnIZD89kxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset(data, train_ratio, val_ratio):\n",
        "    \"\"\"\n",
        "    Chia một list dữ liệu thành ba list con (train, validation, test) dựa trên tỷ lệ.\n",
        "    Tỷ lệ test sẽ được tự động tính toán (1 - train_ratio - val_ratio).\n",
        "    Sử dụng slicing để thực hiện.\n",
        "    \"\"\"\n",
        "    # TODO: Your code here\n",
        "    pass\n",
        "\n",
        "# Test Cases\n",
        "dataset = list(range(100))\n",
        "train, val, test = split_dataset(dataset, 0.7, 0.15)\n",
        "assert (len(train), len(val), len(test)) == (70, 15, 15), \"Test 2.1 Failed\"\n",
        "assert train == list(range(70)), \"Test 2.1 Logic Failed\"\n",
        "assert val == list(range(70, 85)), \"Test 2.1 Logic Failed\"\n",
        "assert test == list(range(85, 100)), \"Test 2.1 Logic Failed\"\n",
        "\n",
        "dataset_2 = list(range(10))\n",
        "train_2, val_2, test_2 = split_dataset(dataset_2, 0.6, 0.2)\n",
        "assert (train_2, val_2, test_2) == (list(range(6)), list(range(6, 8)), list(range(8, 10))), \"Test 2.2 Failed\"\n",
        "\n",
        "dataset_3 = ['a', 'b', 'c']\n",
        "train_3, val_3, test_3 = split_dataset(dataset_3, 0.5, 0.5)\n",
        "assert (len(train_3), len(val_3), len(test_3)) == (1, 1, 1), \"Test 2.3 Failed (rounding)\"\n",
        "\n",
        "dataset_4 = list(range(20))\n",
        "train_4, val_4, test_4 = split_dataset(dataset_4, 1.0, 0.0)\n",
        "assert (train_4, val_4, test_4) == (dataset_4, [], []), \"Test 2.4 Failed\"\n",
        "\n",
        "dataset_5 = []\n",
        "train_5, val_5, test_5 = split_dataset(dataset_5, 0.8, 0.1)\n",
        "assert (train_5, val_5, test_5) == ([], [], []), \"Test 2.5 Failed\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "pLeCWzBU9kxo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Bài tập 3: Mã hóa One-Hot cho Nhãn Dữ liệu**\n",
        "\n",
        "**Bối cảnh:** Các mô hình Machine Learning thường làm việc với các con số, không phải văn bản. Khi chúng ta có các nhãn dạng danh mục (ví dụ: \"cat\", \"dog\", \"bird\"), chúng ta cần chuyển chúng thành dạng vector số. One-Hot Encoding là một phương pháp phổ biến, trong đó mỗi nhãn được biểu diễn bằng một vector có độ dài bằng số lượng lớp, chứa toàn số `0` ngoại trừ một số `1` tại vị trí tương ứng với lớp đó.\n",
        "\n",
        "**Ứng dụng thực tế:** Đây là bước tiền xử lý tiêu chuẩn cho các bài toán phân loại đa lớp trong cả Machine Learning cổ điển và Deep Learning."
      ],
      "metadata": {
        "id": "g1M0egUI9kxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encode(labels, classes):\n",
        "    \"\"\"\n",
        "    Chuyển đổi một list các nhãn văn bản thành list các vector one-hot.\n",
        "    Sử dụng list comprehension và `classes.index()`.\n",
        "    `classes` là một list chứa tất cả các lớp có thể có, đã được sắp xếp.\n",
        "    \"\"\"\n",
        "    # TODO: Your code here\n",
        "    pass\n",
        "\n",
        "# Test Cases\n",
        "all_classes = [\"cat\", \"dog\", \"bird\"]\n",
        "assert one_hot_encode([\"dog\", \"cat\", \"bird\", \"dog\"], all_classes) == [[0, 1, 0], [1, 0, 0], [0, 0, 1], [0, 1, 0]], \"Test 3.1 Failed\"\n",
        "assert one_hot_encode([], all_classes) == [], \"Test 3.2 Failed\"\n",
        "single_class = [\"A\", \"B\"]\n",
        "assert one_hot_encode([\"A\", \"A\", \"B\"], single_class) == [[1, 0], [1, 0], [0, 1]], \"Test 3.3 Failed\"\n",
        "assert one_hot_encode([\"cat\"], all_classes) == [[1, 0, 0]], \"Test 3.4 Failed\"\n",
        "# Test with classes that have more characters\n",
        "emotion_classes = [\"happy\", \"sad\", \"neutral\"]\n",
        "assert one_hot_encode([\"sad\", \"happy\"], emotion_classes) == [[0, 1, 0], [1, 0, 0]], \"Test 3.5 Failed\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "6IQarv0h9kxp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Bài tập 4: Làm phẳng Token để xây dựng Từ điển**\n",
        "\n",
        "**Bối cảnh:** Trong NLP, một bước quan trọng là xây dựng một \"từ điển\" (vocabulary) - một danh sách tất cả các từ duy nhất có trong toàn bộ kho dữ liệu. Dữ liệu đầu vào thường là một danh sách các câu, và mỗi câu lại là một danh sách các từ (tokens). Để xây dựng từ điển, trước tiên chúng ta cần \"làm phẳng\" cấu trúc lồng nhau này thành một danh sách duy nhất chứa tất cả các từ.\n",
        "\n",
        "**Ứng dụng thực tế:** Từ điển là cơ sở để mã hóa văn bản thành số (ví dụ: gán mỗi từ cho một ID duy nhất) trước khi đưa vào các mô hình ngôn ngữ."
      ],
      "metadata": {
        "id": "MFs9NnMc9kxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten_tokens(corpus):\n",
        "    \"\"\"\n",
        "    Làm phẳng một kho văn bản (list của các list từ) thành một list từ duy nhất.\n",
        "    Sử dụng list comprehension lồng nhau.\n",
        "    \"\"\"\n",
        "    # TODO: Your code here\n",
        "    pass\n",
        "\n",
        "# Test Cases\n",
        "corpus1 = [[\"hello\", \"world\"], [\"this\", \"is\", \"a\", \"test\"]]\n",
        "assert flatten_tokens(corpus1) == [\"hello\", \"world\", \"this\", \"is\", \"a\", \"test\"], \"Test 4.1 Failed\"\n",
        "corpus2 = [[\"a\", \"b\"], [\"c\"]]\n",
        "assert flatten_tokens(corpus2) == [\"a\", \"b\", \"c\"], \"Test 4.2 Failed\"\n",
        "corpus3 = []\n",
        "assert flatten_tokens(corpus3) == [], \"Test 4.3 Failed\"\n",
        "corpus4 = [[\"single\", \"sentence\"]]\n",
        "assert flatten_tokens(corpus4) == [\"single\", \"sentence\"], \"Test 4.4 Failed\"\n",
        "corpus5 = [[\"nested\", [\"list\", \"is\"], \"not\", \"supported\"], [\"but\", \"we\", \"assume\", \"flat\", \"sentences\"]]\n",
        "# Giả định các câu không bị lồng sâu hơn 1 cấp\n",
        "assert flatten_tokens(corpus5[:-1]) == [\"nested\", [\"list\", \"is\"], \"not\", \"supported\"], \"Test 4.5 Failed\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "JvnJ5X3G9kxp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Bài tập 5: Lọc các Hộp giới hạn (Bounding Box) tự tin thấp**\n",
        "\n",
        "**Bối cảnh:** Trong các mô hình nhận dạng vật thể (Object Detection) như YOLO hay SSD, đầu ra là một danh sách các \"hộp giới hạn\" (bounding boxes). Mỗi hộp bao gồm tọa độ `[x, y, w, h]`, một nhãn lớp, và một **điểm tự tin (confidence score)** cho biết mức độ chắc chắn của mô hình về dự đoán đó. Để kết quả cuối cùng sạch sẽ, chúng ta cần loại bỏ những dự đoán có độ tự tin quá thấp.\n",
        "\n",
        "**Ứng dụng thực tế:** Kỹ thuật này gọi là \"Confidence Thresholding\", một bước hậu xử lý (post-processing) quan trọng để giảm nhiễu và chỉ giữ lại các phát hiện đáng tin cậy."
      ],
      "metadata": {
        "id": "fvCDQUYa9kxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_low_confidence_boxes(predictions, threshold):\n",
        "    \"\"\"\n",
        "    Lọc và giữ lại các dự đoán (list of lists) có điểm tự tin >= threshold.\n",
        "    Mỗi dự đoán có dạng: [class_id, confidence, x, y, w, h]\n",
        "    Sử dụng list comprehension với điều kiện `if`.\n",
        "    \"\"\"\n",
        "    # TODO: Your code here\n",
        "    pass\n",
        "\n",
        "# Test Cases\n",
        "predictions1 = [[0, 0.95, 10, 10, 50, 50], [1, 0.4, 20, 20, 30, 30], [0, 0.88, 15, 15, 40, 40]]\n",
        "assert filter_low_confidence_boxes(predictions1, 0.8) == [[0, 0.95, 10, 10, 50, 50], [0, 0.88, 15, 15, 40, 40]], \"Test 5.1 Failed\"\n",
        "assert filter_low_confidence_boxes(predictions1, 0.99) == [], \"Test 5.2 Failed\"\n",
        "assert filter_low_confidence_boxes([], 0.5) == [], \"Test 5.3 Failed\"\n",
        "predictions4 = [[0, 0.9], [1, 0.8], [2, 0.7]]\n",
        "assert filter_low_confidence_boxes(predictions4, 0.8) == [[0, 0.9], [1, 0.8]], \"Test 5.4 Failed\"\n",
        "assert filter_low_confidence_boxes(predictions4, 0.0) == predictions4, \"Test 5.5 Failed\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "2YOdU2fn9kxp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Bài tập 6: Chuẩn hóa Dữ liệu Đặc trưng (Feature Scaling)**\n",
        "\n",
        "**Bối cảnh:** Các đặc trưng (features) trong một tập dữ liệu thường có các thang đo rất khác nhau (ví dụ: tuổi từ 18-65, thu nhập từ 10M-1B). Điều này có thể làm cho các thuật toán Machine Learning (như SVM, K-Means) hoạt động không hiệu quả. Một phương pháp phổ biến để giải quyết vấn đề này là chuẩn hóa Min-Max, đưa tất cả các giá trị về khoảng `[0, 1]`.\n",
        "\n",
        "**Ứng dụng thực tế:** Feature Scaling là một bước tiền xử lý quan trọng giúp các thuật toán hội tụ nhanh hơn và cải thiện hiệu suất của mô hình."
      ],
      "metadata": {
        "id": "BRrsPf-a9kxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_feature(feature_data):\n",
        "    \"\"\"\n",
        "    Chuẩn hóa một list dữ liệu (một đặc trưng) về khoảng [0, 1] bằng công thức Min-Max.\n",
        "    x_norm = (x - min) / (max - min)\n",
        "    Sử dụng list comprehension. Xử lý trường hợp max == min.\n",
        "    \"\"\"\n",
        "    # TODO: Your code here\n",
        "    pass\n",
        "\n",
        "# Test Cases\n",
        "assert normalize_feature([10, 20, 30, 40, 50]) == [0.0, 0.25, 0.5, 0.75, 1.0], \"Test 6.1 Failed\"\n",
        "assert normalize_feature([150, 100, 200]) == [0.5, 0.0, 1.0], \"Test 6.2 Failed\"\n",
        "assert normalize_feature([5, 5, 5, 5]) == [0.0, 0.0, 0.0, 0.0], \"Test 6.3 Failed\"\n",
        "assert normalize_feature([]) == [], \"Test 6.4 Failed\"\n",
        "assert normalize_feature([-5, 0, 5]) == [0.0, 0.5, 1.0], \"Test 6.5 Failed\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "4VJCq6U19kxq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Bài tập 7: Tính toán Độ chính xác (Accuracy) của Mô hình**\n",
        "\n",
        "**Bối cảnh:** Sau khi huấn luyện một mô hình phân loại, chúng ta cần đo lường xem nó hoạt động tốt đến mức nào. Độ chính xác (Accuracy) là một trong những thước đo đơn giản nhất: nó là tỷ lệ phần trăm số điểm dữ liệu được mô hình dự đoán đúng.\n",
        "\n",
        "**Ứng dụng thực tế:** Accuracy cung cấp một cái nhìn tổng quan nhanh chóng về hiệu suất của mô hình, mặc dù nó có thể không phù hợp cho các tập dữ liệu mất cân bằng."
      ],
      "metadata": {
        "id": "y3c_Jpeu9kxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(true_labels, predicted_labels):\n",
        "    \"\"\"\n",
        "    Tính độ chính xác của mô hình từ hai list nhãn.\n",
        "    Sử dụng `zip` để ghép cặp nhãn và list comprehension (hoặc generator) để đếm số dự đoán đúng.\n",
        "    \"\"\"\n",
        "    # TODO: Your code here\n",
        "    pass\n",
        "\n",
        "# Test Cases\n",
        "y_true_1 = [1, 0, 1, 1, 0]\n",
        "y_pred_1 = [1, 0, 0, 1, 0]\n",
        "assert calculate_accuracy(y_true_1, y_pred_1) == 0.8, \"Test 7.1 Failed\"\n",
        "\n",
        "y_true_2 = [\"cat\", \"dog\", \"cat\", \"bird\"]\n",
        "y_pred_2 = [\"cat\", \"cat\", \"cat\", \"bird\"]\n",
        "assert calculate_accuracy(y_true_2, y_pred_2) == 0.75, \"Test 7.2 Failed\"\n",
        "\n",
        "y_true_3 = [0, 0, 0]\n",
        "y_pred_3 = [1, 1, 1]\n",
        "assert calculate_accuracy(y_true_3, y_pred_3) == 0.0, \"Test 7.3 Failed\"\n",
        "\n",
        "y_true_4 = [1, 2, 3]\n",
        "y_pred_4 = [1, 2, 3]\n",
        "assert calculate_accuracy(y_true_4, y_pred_4) == 1.0, \"Test 7.4 Failed\"\n",
        "\n",
        "assert calculate_accuracy([], []) == 0.0, \"Test 7.5 Failed\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "6XpFcLqs9kxq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Bài tập 8: Tăng cường Dữ liệu Chuỗi thời gian (Time-Series Augmentation)**\n",
        "\n",
        "**Bối cảnh:** Một trong những thách thức lớn nhất trong Deep Learning là thiếu dữ liệu. Tăng cường dữ liệu (Data Augmentation) là một kỹ thuật tạo ra các mẫu dữ liệu mới từ dữ liệu hiện có. Đối với dữ liệu chuỗi thời gian (ví dụ: tín hiệu ECG, giá cổ phiếu), một kỹ thuật đơn giản là đảo ngược chuỗi.\n",
        "\n",
        "**Ứng dụng thực tế:** Giúp mô hình trở nên mạnh mẽ hơn (robust) và khái quát hóa tốt hơn bằng cách học các biểu diễn bất biến với hướng của thời gian."
      ],
      "metadata": {
        "id": "3mywYupb9kxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_time_series(data, indices_to_reverse):\n",
        "    \"\"\"\n",
        "    Tạo một bản sao của dữ liệu và tăng cường nó bằng cách đảo ngược các chuỗi tại các chỉ số cho trước.\n",
        "    Sử dụng slicing `[::-1]` để đảo ngược.\n",
        "    \"\"\"\n",
        "    # TODO: Your code here\n",
        "    pass\n",
        "\n",
        "# Test Cases\n",
        "data1 = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
        "assert augment_time_series(data1, [0, 2]) == [[3, 2, 1], [4, 5, 6], [9, 8, 7]], \"Test 8.1 Failed\"\n",
        "original_data = [[1,2], [3,4]]\n",
        "augmented = augment_time_series(original_data, [1])\n",
        "assert original_data == [[1,2], [3,4]], \"Test 8.2 Failed (original data was modified)\"\n",
        "assert augmented == [[1,2], [4,3]], \"Test 8.2 Logic Failed\"\n",
        "assert augment_time_series([[10, 20]], [0]) == [[20, 10]], \"Test 8.3 Failed\"\n",
        "assert augment_time_series([[1, 2], [3, 4]], []) == [[1, 2], [3, 4]], \"Test 8.4 Failed\"\n",
        "assert augment_time_series([], [0, 1]) == [], \"Test 8.5 Failed\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "kqtNjNpN9kxq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Bài tập 9: Quản lý Bộ đệm Kinh nghiệm (Experience Replay Buffer)**\n",
        "\n",
        "**Bối cảnh:** Trong Học tăng cường (Reinforcement Learning), một agent tương tác với môi trường và thu thập \"kinh nghiệm\" (trạng thái, hành động, phần thưởng, trạng thái tiếp theo). Thay vì học ngay lập tức từ kinh nghiệm mới nhất, agent lưu trữ chúng trong một \"bộ đệm\" có kích thước cố định và lấy mẫu ngẫu nhiên từ đó để học. Điều này giúp phá vỡ sự tương quan giữa các kinh nghiệm liên tiếp.\n",
        "\n",
        "**Ứng dụng thực tế:** Kỹ thuật Experience Replay là chìa khóa thành công của các thuật toán như Deep Q-Networks (DQN). `collections.deque` là cấu trúc dữ liệu hoàn hảo cho việc này."
      ],
      "metadata": {
        "id": "TWGsbY0g9kxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import deque\n",
        "\n",
        "class ExperienceReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def add(self, experience):\n",
        "        \"\"\"Thêm một kinh nghiệm mới vào bộ đệm.\"\"\"\n",
        "        # TODO: Your code here\n",
        "        pass\n",
        "\n",
        "    def get_buffer_as_list(self):\n",
        "        \"\"\"Trả về nội dung của bộ đệm dưới dạng một list.\"\"\"\n",
        "        # TODO: Your code here\n",
        "        pass\n",
        "\n",
        "# Test Cases\n",
        "buffer = ExperienceReplayBuffer(3)\n",
        "buffer.add(\"exp1\")\n",
        "buffer.add(\"exp2\")\n",
        "assert buffer.get_buffer_as_list() == [\"exp1\", \"exp2\"], \"Test 9.1 Failed\"\n",
        "buffer.add(\"exp3\")\n",
        "assert buffer.get_buffer_as_list() == [\"exp1\", \"exp2\", \"exp3\"], \"Test 9.2 Failed\"\n",
        "buffer.add(\"exp4\") # \"exp1\" should be pushed out\n",
        "assert buffer.get_buffer_as_list() == [\"exp2\", \"exp3\", \"exp4\"], \"Test 9.3 Failed\"\n",
        "buffer.add(\"exp5\"); buffer.add(\"exp6\")\n",
        "assert buffer.get_buffer_as_list() == [\"exp4\", \"exp5\", \"exp6\"], \"Test 9.4 Failed\"\n",
        "buffer_empty = ExperienceReplayBuffer(2)\n",
        "assert buffer_empty.get_buffer_as_list() == [], \"Test 9.5 Failed\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "pdJ_uA909kxr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Bài tập 10: Vector hóa Bag-of-Words**\n",
        "\n",
        "**Bối cảnh:** Bag-of-Words (BoW) là một cách đơn giản để biểu diễn văn bản dưới dạng vector số. Một vector BoW có độ dài bằng kích thước của từ điển. Mỗi phần tử của vector cho biết tần suất xuất hiện của từ tương ứng trong từ điển đối với một câu cụ thể.\n",
        "\n",
        "**Ứng dụng thực tế:** BoW là một kỹ thuật feature engineering kinh điển trong NLP, được sử dụng cho các tác vụ như phân loại văn bản, phân tích cảm xúc trước khi có các mô hình phức tạp hơn như Word2Vec hay BERT. Việc sử dụng `set` để tra cứu từ trong từ điển sẽ hiệu quả hơn `list`."
      ],
      "metadata": {
        "id": "6o9oWrTh9kxr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_bow(sentence, vocabulary):\n",
        "    \"\"\"\n",
        "    Chuyển đổi một câu (list các từ) thành một vector Bag-of-Words.\n",
        "    Sử dụng một cấu trúc dữ liệu hiệu quả (set) cho việc tra cứu từ trong từ điển.\n",
        "    \"\"\"\n",
        "    # TODO: Your code here\n",
        "    pass\n",
        "\n",
        "# Test Cases\n",
        "vocab = [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"lazy\", \"dog\"]\n",
        "sentence1 = [\"the\", \"dog\", \"was\", \"lazy\"]\n",
        "assert vectorize_bow(sentence1, vocab) == [1, 0, 0, 0, 0, 0, 1, 1], \"Test 10.1 Failed\"\n",
        "sentence2 = [\"a\", \"quick\", \"brown\", \"cat\"]\n",
        "assert vectorize_bow(sentence2, vocab) == [0, 1, 1, 0, 0, 0, 0, 0], \"Test 10.2 Failed\"\n",
        "assert vectorize_bow([], vocab) == [0, 0, 0, 0, 0, 0, 0, 0], \"Test 10.3 Failed\"\n",
        "sentence4 = [\"the\", \"the\", \"the\"]\n",
        "assert vectorize_bow(sentence4, vocab) == [3, 0, 0, 0, 0, 0, 0, 0], \"Test 10.4 Failed\"\n",
        "assert vectorize_bow([\"hello\", \"world\"], []) == [], \"Test 10.5 Failed\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "EU7fJi6m9kxr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "\n",
        "## **Đáp án và Giải thích**\n",
        "\n",
        "Dưới đây là lời giải gợi ý cùng phần giải thích chi tiết cho 10 bài tập trên.\n",
        "\n",
        "### **Đáp án 1: Tạo và Đệm (Padding) cho các Batch Dữ liệu**"
      ],
      "metadata": {
        "id": "3T93s8Bj9kxr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_padded_batch(data, batch_size):\n",
        "    if not data:\n",
        "        return []\n",
        "\n",
        "    batches = []\n",
        "    # Chia dữ liệu thành các batch\n",
        "    for i in range(0, len(data), batch_size):\n",
        "        batch = data[i:i + batch_size]\n",
        "\n",
        "        # Tìm độ dài tối đa trong batch hiện tại\n",
        "        max_len = 0\n",
        "        for seq in batch:\n",
        "            if len(seq) > max_len:\n",
        "                max_len = len(seq)\n",
        "\n",
        "        # Đệm cho từng sequence trong batch\n",
        "        padded_batch = []\n",
        "        for seq in batch:\n",
        "            # Phép cộng list và nhân list được sử dụng ở đây\n",
        "            padded_seq = seq + [0] * (max_len - len(seq))\n",
        "            padded_batch.append(padded_seq)\n",
        "\n",
        "        batches.append(padded_batch)\n",
        "\n",
        "    return batches"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "fnf9GcjW9kxr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Giải thích:**\n",
        "1.  Hàm lặp qua dữ liệu với bước nhảy là `batch_size` để tạo ra các \"lô\" con. Kỹ thuật slicing `data[i:i + batch_size]` được dùng để trích xuất từng lô.\n",
        "2.  Bên trong mỗi lô, chúng ta tìm `max_len`, là độ dài của câu dài nhất.\n",
        "3.  Sau đó, chúng ta duyệt qua từng câu (`seq`) trong lô đó.\n",
        "4.  Dòng `padded_seq = seq + [0] * (max_len - len(seq))` là mấu chốt:\n",
        "    * `(max_len - len(seq))` tính toán số lượng giá trị `0` cần thêm vào.\n",
        "    * `[0] * ...` tạo ra một list chứa đúng số lượng `0` cần thiết.\n",
        "    * Phép cộng `+` nối list câu gốc với list các số `0`, tạo ra câu đã được đệm.\n",
        "5.  Cuối cùng, các lô đã được đệm được thu thập lại và trả về.\n",
        "\n",
        "---\n",
        "\n",
        "### **Đáp án 2: Chia Tập dữ liệu (Train-Validation-Test Split)**"
      ],
      "metadata": {
        "id": "_T5ql_zz9kxr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset(data, train_ratio, val_ratio):\n",
        "    n = len(data)\n",
        "    train_end = int(n * train_ratio)\n",
        "    val_end = train_end + int(n * val_ratio)\n",
        "\n",
        "    # Slicing để lấy các phần của list\n",
        "    train_set = data[:train_end]\n",
        "    val_set = data[train_end:val_end]\n",
        "    test_set = data[val_end:]\n",
        "\n",
        "    return train_set, val_set, test_set"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "HO2cg1Jq9kxr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Giải thích:**\n",
        "1.  Hàm tính toán các chỉ số điểm cuối cho tập train và tập validation dựa trên tổng số lượng dữ liệu (`n`) và các tỷ lệ cho trước. `int()` được dùng để đảm bảo chỉ số là số nguyên.\n",
        "2.  **`data[:train_end]`**: Đây là cú pháp slicing, tạo ra một list mới từ đầu `data` cho đến *ngay trước* chỉ số `train_end`. Đây chính là tập huấn luyện.\n",
        "3.  **`data[train_end:val_end]`**: Cú pháp này cắt một lát từ chỉ số `train_end` đến *ngay trước* `val_end`, tạo ra tập kiểm định.\n",
        "4.  **`data[val_end:]`**: Cú pháp này cắt một lát từ chỉ số `val_end` cho đến hết list, tạo ra tập kiểm tra.\n",
        "5.  Phương pháp này tận dụng khả năng slicing hiệu quả của `list` để phân chia dữ liệu một cách ngắn gọn và dễ đọc.\n",
        "\n",
        "---\n",
        "\n",
        "### **Đáp án 3: Mã hóa One-Hot cho Nhãn Dữ liệu**"
      ],
      "metadata": {
        "id": "HeN_n98I9kxr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encode(labels, classes):\n",
        "    num_classes = len(classes)\n",
        "    # Cách viết Pythonic hơn sử dụng list comprehension lồng nhau:\n",
        "    return [[1 if i == classes.index(label) else 0 for i in range(num_classes)] for label in labels]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "7kTy7x6i9kxr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Giải thích:**\n",
        "1.  Hàm này sử dụng một list comprehension lồng nhau, một kỹ thuật rất mạnh mẽ và \"Pythonic\".\n",
        "2.  **`for label in labels`**: Vòng lặp ngoài duyệt qua từng nhãn trong `labels` (ví dụ: \"dog\", \"cat\").\n",
        "3.  **`[... for i in range(num_classes)]`**: Vòng lặp trong tạo ra một vector mới cho mỗi `label`.\n",
        "4.  **`classes.index(label)`**: Tìm chỉ số của `label` hiện tại trong danh sách `classes`. Ví dụ, nếu `classes` là `[\"cat\", \"dog\", \"bird\"]` và `label` là `\"dog\"`, `index` sẽ là `1`.\n",
        "5.  **`1 if i == classes.index(label) else 0`**: Đây là một biểu thức điều kiện (ternary operator). Với mỗi vị trí `i` trong vector mới, nó sẽ gán giá trị `1` nếu `i` trùng với chỉ số của `label`, ngược lại gán `0`.\n",
        "6.  Kết quả là một list các vector one-hot được tạo ra một cách cực kỳ tinh gọn.\n",
        "\n",
        "---\n",
        "\n",
        "### **Đáp án 4: Làm phẳng Token để xây dựng Từ điển**"
      ],
      "metadata": {
        "id": "GLJQRA3v9kxr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten_tokens(corpus):\n",
        "    return [token for sentence in corpus for token in sentence]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "amWhoVol9kxr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Giải thích:**\n",
        "1.  Đây là một ví dụ kinh điển của list comprehension lồng nhau để làm phẳng một list 2D.\n",
        "2.  **`for sentence in corpus`**: Vòng lặp ngoài cùng, tương đương với `for sentence in corpus:`.\n",
        "3.  **`for token in sentence`**: Vòng lặp bên trong, tương đương với `for token in sentence:`.\n",
        "4.  **`token`**: Biểu thức ở đầu, lấy mỗi `token` từ vòng lặp trong cùng và đưa nó vào list kết quả.\n",
        "5.  Cách đọc của cú pháp này theo đúng thứ tự của vòng lặp lồng nhau: \"Lấy mỗi `token` cho mỗi `sentence` trong `corpus`, và cho mỗi `token` trong `sentence` đó\". Điều này hiệu quả hơn nhiều so với việc dùng hai vòng `for` và `append` vào một list mới.\n",
        "\n",
        "---\n",
        "\n",
        "### **Đáp án 5: Lọc các Hộp giới hạn (Bounding Box) tự tin thấp**"
      ],
      "metadata": {
        "id": "LWwE__G49kxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_low_confidence_boxes(predictions, threshold):\n",
        "    # Mỗi prediction có dạng: [class_id, confidence, x, y, w, h]\n",
        "    # Phần tử ở chỉ số 1 là confidence score\n",
        "    return [pred for pred in predictions if pred[1] >= threshold]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "PLVIp9-Y9kxs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Giải thích:**\n",
        "1.  Hàm sử dụng list comprehension, một cách viết ngắn gọn để tạo ra một list mới dựa trên một list cũ.\n",
        "2.  **`[pred for pred in predictions ...]`**: Cấu trúc này có nghĩa là \"tạo một list mới bằng cách lấy mỗi `pred` từ `predictions`...\".\n",
        "3.  **`... if pred[1] >= threshold`**: Đây là mệnh đề lọc. Chỉ những `pred` nào thỏa mãn điều kiện này mới được đưa vào list kết quả.\n",
        "4.  `pred[1]` truy cập vào phần tử thứ hai (điểm tự tin) của mỗi dự đoán `pred`.\n",
        "5.  Kết quả là một list mới chỉ chứa những dự đoán có độ tự tin đủ cao, được thực hiện trong một dòng code duy nhất.\n",
        "\n",
        "---\n",
        "\n",
        "### **Đáp án 6: Chuẩn hóa Dữ liệu Đặc trưng (Feature Scaling)**"
      ],
      "metadata": {
        "id": "SEuj2o029kxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_feature(feature_data):\n",
        "    if not feature_data:\n",
        "        return []\n",
        "\n",
        "    min_val = min(feature_data)\n",
        "    max_val = max(feature_data)\n",
        "\n",
        "    range_val = max_val - min_val\n",
        "    # Xử lý trường hợp tất cả các phần tử đều bằng nhau\n",
        "    if range_val == 0:\n",
        "        return [0.0] * len(feature_data)\n",
        "\n",
        "    # Áp dụng công thức chuẩn hóa cho mỗi phần tử bằng list comprehension\n",
        "    return [(x - min_val) / range_val for x in feature_data]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "zGnYqn3F9kxs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Giải thích:**\n",
        "1.  Đầu tiên, hàm xử lý các trường hợp biên: nếu list rỗng, trả về list rỗng.\n",
        "2.  Sử dụng hàm `min()` và `max()` để tìm giá trị nhỏ nhất và lớn nhất trong list.\n",
        "3.  Tính `range_val`. Nếu `range_val` bằng 0 (nghĩa là tất cả các phần tử trong list đều bằng nhau), ta không thể chia cho 0. Trong trường hợp này, tất cả các giá trị chuẩn hóa đều là `0.0`.\n",
        "4.  Dòng cuối cùng là một list comprehension, áp dụng công thức chuẩn hóa $x_{norm} = \\frac{x - min}{max - min}$ cho mỗi phần tử `x` trong `feature_data` và tạo ra một list kết quả mới.\n",
        "\n",
        "---\n",
        "\n",
        "### **Đáp án 7: Tính toán Độ chính xác (Accuracy) của Mô hình**"
      ],
      "metadata": {
        "id": "5uNd3nQ29kxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(true_labels, predicted_labels):\n",
        "    if not true_labels:\n",
        "        return 0.0\n",
        "\n",
        "    # Dùng generator expression bên trong sum() để hiệu quả hơn\n",
        "    correct_predictions = sum(1 for true, pred in zip(true_labels, predicted_labels) if true == pred)\n",
        "\n",
        "    return correct_predictions / len(true_labels)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "LimjMSv69kxs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Giải thích:**\n",
        "1.  Hàm `zip(true_labels, predicted_labels)` tạo ra một iterator ghép cặp từng phần tử của hai list, ví dụ: `(true_label_1, pred_label_1)`, `(true_label_2, pred_label_2)`, ...\n",
        "2.  `1 for true, pred in ... if true == pred` là một **generator expression**. Nó hoạt động tương tự list comprehension nhưng không tạo ra một list đầy đủ trong bộ nhớ, giúp tiết kiệm bộ nhớ. Nó sẽ sinh ra một giá trị `1` cho mỗi cặp nhãn `(true, pred)` giống hệt nhau.\n",
        "3.  `sum(...)` tính tổng tất cả các số `1` được sinh ra, cho chúng ta tổng số các dự đoán đúng.\n",
        "4.  Cuối cùng, chia tổng số dự đoán đúng cho tổng số nhãn (`len(true_labels)`) để ra tỷ lệ chính xác.\n",
        "\n",
        "---\n",
        "\n",
        "### **Đáp án 8: Tăng cường Dữ liệu Chuỗi thời gian (Time-Series Augmentation)**"
      ],
      "metadata": {
        "id": "T2oDO4UZ9kxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_time_series(data, indices_to_reverse):\n",
        "    # Tạo một bản sao nông của list gốc bằng slicing `[:]`\n",
        "    # để không làm thay đổi dữ liệu ban đầu.\n",
        "    augmented_data = data[:]\n",
        "\n",
        "    for i in indices_to_reverse:\n",
        "        if 0 <= i < len(augmented_data):\n",
        "            # Thay thế phần tử trong bản sao bằng một list mới đã đảo ngược.\n",
        "            # `augmented_data[i][::-1]` tạo ra một list con đảo ngược MỚI.\n",
        "            augmented_data[i] = augmented_data[i][::-1]\n",
        "\n",
        "    return augmented_data"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "8SGhL_xD9kxs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Giải thích:**\n",
        "1.  **`augmented_data = data[:]`**: Đây là bước quan trọng. Nó tạo ra một **bản sao nông (shallow copy)** của list `data`. Điều này đảm bảo rằng khi chúng ta thay đổi `augmented_data`, list `data` gốc sẽ không bị ảnh hưởng.\n",
        "2.  Hàm duyệt qua các chỉ số `i` cần đảo ngược.\n",
        "3.  **`augmented_data[i][::-1]`**: Đây là cú pháp slicing thần thánh để đảo ngược một list. `[::-1]` có nghĩa là bắt đầu từ đầu đến cuối, với bước nhảy là `-1`, tức là đi ngược lại. Nó tạo ra một list con mới đã được đảo ngược.\n",
        "4.  **`augmented_data[i] = ...`**: Phép gán này thay thế list con cũ tại vị trí `i` trong `augmented_data` bằng list con mới đã đảo ngược.\n",
        "\n",
        "---\n",
        "\n",
        "### **Đáp án 9: Quản lý Bộ đệm Kinh nghiệm (Experience Replay Buffer)**"
      ],
      "metadata": {
        "id": "exNufk4P9kxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from collections import deque (đã có ở phần bài tập)\n",
        "\n",
        "class ExperienceReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def add(self, experience):\n",
        "        \"\"\"Thêm một kinh nghiệm mới vào bộ đệm.\"\"\"\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def get_buffer_as_list(self):\n",
        "        \"\"\"Trả về nội dung của bộ đệm dưới dạng một list.\"\"\"\n",
        "        return list(self.buffer)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "TXtvcIgF9kxs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Giải thích:**\n",
        "1.  **`self.buffer = deque(maxlen=capacity)`**: Thay vì dùng `list`, chúng ta dùng `collections.deque` với tham số `maxlen`. `deque` là một hàng đợi hai đầu, được tối ưu cho việc thêm/xóa phần tử ở cả hai đầu (độ phức tạp O(1)).\n",
        "2.  Khi `maxlen` được thiết lập, `deque` tự động trở thành một bộ đệm có kích thước cố định.\n",
        "3.  **`self.buffer.append(experience)`**: Phương thức `add` chỉ đơn giản là gọi `append`. Khi bộ đệm đã đầy, việc `append` một phần tử mới sẽ **tự động loại bỏ phần tử ở đầu kia (phần tử cũ nhất)**. Đây chính là hành vi của bộ đệm xoay vòng mà chúng ta cần, được `deque` xử lý một cách hiệu quả.\n",
        "4.  **`return list(self.buffer)`**: Để trả về một `list` thông thường theo yêu cầu, chúng ta chỉ cần dùng hàm khởi tạo `list()` để chuyển đổi `deque`.\n",
        "\n",
        "---\n",
        "\n",
        "### **Đáp án 10: Vector hóa Bag-of-Words**"
      ],
      "metadata": {
        "id": "P8yd7ifR9kxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_bow(sentence, vocabulary):\n",
        "    if not vocabulary:\n",
        "        return []\n",
        "\n",
        "    # Chuyển vocabulary thành set để tra cứu O(1) thay vì O(n) của list\n",
        "    vocab_set = set(vocabulary)\n",
        "\n",
        "    # Tạo một dict để map từ vựng với chỉ số của nó, giúp tra cứu O(1)\n",
        "    vocab_indices = {word: i for i, word in enumerate(vocabulary)}\n",
        "\n",
        "    # Khởi tạo vector kết quả\n",
        "    vector = [0] * len(vocabulary)\n",
        "    for word in sentence:\n",
        "        # Kiểm tra từ có trong từ điển không một cách hiệu quả\n",
        "        if word in vocab_set:\n",
        "            index = vocab_indices[word]\n",
        "            vector[index] += 1\n",
        "\n",
        "    return vector"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "jO8FRb6O9kxw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Giải thích:**\n",
        "1.  **Tối ưu hóa hiệu suất**: Thay vì tìm kiếm trong `vocabulary` (một `list`, có độ phức tạp `O(n)`), chúng ta chuyển nó thành `vocab_set` (một `set`) và `vocab_indices` (một `dict`). Việc tra cứu `word in vocab_set` và `vocab_indices[word]` đều có độ phức tạp trung bình là `O(1)`, nhanh hơn rất nhiều khi từ điển lớn.\n",
        "2.  **`vector = [0] * len(vocabulary)`**: Tạo ra một list chứa các số `0` có độ dài bằng kích thước của từ điển.\n",
        "3.  Hàm duyệt qua từng `word` trong `sentence`.\n",
        "4.  Nếu `word` tồn tại trong `vocab_set`, chúng ta lấy chỉ số của nó từ `vocab_indices` và tăng giá trị tại chỉ số đó trong `vector` lên `1`.\n",
        "5.  Kết quả là một vector tần suất, biểu diễn cho câu đầu vào."
      ],
      "metadata": {
        "id": "P3CXabvh9kxw"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}